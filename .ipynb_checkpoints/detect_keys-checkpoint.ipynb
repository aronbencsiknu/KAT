{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c8432365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# --importing libraries below--\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6ad36d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"Audio\"\n",
    "JSON_PATH = \"Json\\\\data.json\"\n",
    "SAMPLE_RATE = 22050\n",
    "#PREDICT_DATASET_PATH = \"Prediction-Audio\\\\abcdefg.wav\"\n",
    "Predict_JSON_PATH = \"Json\\\\predict.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d637d434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_data(DATASET_PATH, JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a8affc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_20 (Flatten)        (None, 78)                0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 400)               31600     \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 128)               51328     \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91,444\n",
      "Trainable params: 91,444\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 1s 21ms/step - loss: 85.1434 - accuracy: 0.3180 - val_loss: 9.2517 - val_accuracy: 0.3214\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 8.8310 - accuracy: 0.3333 - val_loss: 6.1190 - val_accuracy: 0.3750\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5041 - accuracy: 0.4061 - val_loss: 1.6056 - val_accuracy: 0.4196\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 1.0899 - accuracy: 0.6015 - val_loss: 0.9567 - val_accuracy: 0.6161\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.7018 - accuracy: 0.7088 - val_loss: 0.4676 - val_accuracy: 0.7946\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.3624 - accuracy: 0.8506 - val_loss: 0.4427 - val_accuracy: 0.8482\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.2143 - accuracy: 0.9464 - val_loss: 0.3405 - val_accuracy: 0.8929\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.1522 - accuracy: 0.9579 - val_loss: 0.3672 - val_accuracy: 0.9107\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.1485 - accuracy: 0.9502 - val_loss: 0.3184 - val_accuracy: 0.9196\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9425 - val_loss: 0.2650 - val_accuracy: 0.9286\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1525 - accuracy: 0.9425 - val_loss: 0.2383 - val_accuracy: 0.9286\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0883 - accuracy: 0.9732 - val_loss: 0.2668 - val_accuracy: 0.9196\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9732 - val_loss: 0.2283 - val_accuracy: 0.9554\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9808 - val_loss: 0.2066 - val_accuracy: 0.9464\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0418 - accuracy: 0.9847 - val_loss: 0.2142 - val_accuracy: 0.9643\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0321 - accuracy: 0.9847 - val_loss: 0.1726 - val_accuracy: 0.9464\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 0.9923 - val_loss: 0.1776 - val_accuracy: 0.9732\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 0.9923 - val_loss: 0.1636 - val_accuracy: 0.9643\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0643 - accuracy: 0.9770 - val_loss: 0.2946 - val_accuracy: 0.8929\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9808 - val_loss: 0.1597 - val_accuracy: 0.9554\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 0.1692 - val_accuracy: 0.9554\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 0.9962 - val_loss: 0.2282 - val_accuracy: 0.9196\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0377 - accuracy: 0.9885 - val_loss: 0.1499 - val_accuracy: 0.9554\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.1994 - val_accuracy: 0.9464\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.1824 - val_accuracy: 0.9464\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9375\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1769 - val_accuracy: 0.9554\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1682 - val_accuracy: 0.9554\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1724 - val_accuracy: 0.9554\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1649 - val_accuracy: 0.9554\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1690 - val_accuracy: 0.9464\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1713 - val_accuracy: 0.9464\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1674 - val_accuracy: 0.9464\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 9.6665e-04 - accuracy: 1.0000 - val_loss: 0.1652 - val_accuracy: 0.9464\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 8.6041e-04 - accuracy: 1.0000 - val_loss: 0.1663 - val_accuracy: 0.9464\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7.8540e-04 - accuracy: 1.0000 - val_loss: 0.1673 - val_accuracy: 0.9464\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7.2446e-04 - accuracy: 1.0000 - val_loss: 0.1656 - val_accuracy: 0.9643\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7.0953e-04 - accuracy: 1.0000 - val_loss: 0.1674 - val_accuracy: 0.9464\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6.3873e-04 - accuracy: 1.0000 - val_loss: 0.1674 - val_accuracy: 0.9464\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 5.8585e-04 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9464\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 5.9968e-04 - accuracy: 1.0000 - val_loss: 0.1740 - val_accuracy: 0.9464\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 5.5987e-04 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9554\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 5.1329e-04 - accuracy: 1.0000 - val_loss: 0.1692 - val_accuracy: 0.9554\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4.6868e-04 - accuracy: 1.0000 - val_loss: 0.1702 - val_accuracy: 0.9464\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4.4529e-04 - accuracy: 1.0000 - val_loss: 0.1705 - val_accuracy: 0.9464\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4.3196e-04 - accuracy: 1.0000 - val_loss: 0.1704 - val_accuracy: 0.9554\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4.4112e-04 - accuracy: 1.0000 - val_loss: 0.1691 - val_accuracy: 0.9643\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4.0546e-04 - accuracy: 1.0000 - val_loss: 0.1741 - val_accuracy: 0.9464\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 3.9925e-04 - accuracy: 1.0000 - val_loss: 0.1735 - val_accuracy: 0.9464\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 3.6611e-04 - accuracy: 1.0000 - val_loss: 0.1713 - val_accuracy: 0.9554\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8dc76e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted index: [2 2 0 0 1 1] predicted mapping: ['space' 'space' 'e' 'e' 'q' 'q']\n"
     ]
    }
   ],
   "source": [
    "tmp = save_data_to_predict(\"Prediction-Audio\\\\test.wav\", Predict_JSON_PATH)\n",
    "data = np.array(tmp[\"mfcc\"])\n",
    "mapping = load_mapping_for_predict(JSON_PATH)\n",
    "\n",
    "#Make predictions\n",
    "predictions = model.predict(data)\n",
    "\n",
    "predicted_index = np.argmax(predictions, axis=1)\n",
    "print(\"predicted index: {} predicted mapping: {}\".format(predicted_index, mapping[predicted_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e5af3",
   "metadata": {},
   "source": [
    "<h2>Getting sum of FFT of one 10ms segment.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "daa55d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fftAndSum(signal):\n",
    "    \n",
    "    #Transform waveform into frequency domain using fast Fourier transform\n",
    "    X = np.fft.fft(signal)\n",
    "    \n",
    "    #Get 'loudness' of the 10 ms interval\n",
    "    X_mag = np.absolute(X)\n",
    "    sum_of_fft = sum(X_mag)\n",
    "    return sum_of_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86952333",
   "metadata": {},
   "source": [
    "<h2>Looping through 10ms segments and getting FFT sums and detecting keypresses.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b1586ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_touch_peaks(signal, sr):\n",
    "    \n",
    "    # splitting sound into 10 ms chunks\n",
    "    soundLength = len(signal) / sr # length in seconds\n",
    "    howManyParts = soundLength * 100 # how many chunks to cut up into\n",
    "    tenMsParts = np.array_split(signal, howManyParts) # create new array containing 10ms parts\n",
    "    \n",
    "    currentTime = 0\n",
    "    energyArray = []\n",
    "    detectedClicks = []\n",
    "    detectedClicksVisualize = [] #Used for plotting FFTs\n",
    "    ArrayOfMss = []\n",
    "    passedFirstPeak = False\n",
    "    detectedKeyPressTime = 0\n",
    "\n",
    "    for oneSegment in tenMsParts:\n",
    "        \n",
    "        ArrayOfMss.append(currentTime) # append current time\n",
    "        sum_of_fft = round(fftAndSum(oneSegment)) # call fftAndSum and store returned value\n",
    "        energyArray.append(sum_of_fft)\n",
    "\n",
    "        #sum_of_fft > (Threshold) which is considered as a peak\n",
    "        if(sum_of_fft > 50 and not(passedFirstPeak)):\n",
    "            \n",
    "            #passedFirstPeak bool is used to skip over the button's release peak when detected the touch peak\n",
    "            passedFirstPeak = True\n",
    "            detectedKeyPressTime = currentTime\n",
    "            \n",
    "            detectedClicks.append(currentTime) # add time of detected click\n",
    "            detectedClicksVisualize.append(int(currentTime/10))\n",
    "            \n",
    "        elif(currentTime - detectedKeyPressTime > 200): # wait x milliseconds after first peak\n",
    "            passedFirstPeak = False\n",
    "            \n",
    "        currentTime += 10\n",
    "        \n",
    "    #plt.figure(figsize=(50, 8))\n",
    "    #plt.plot(ArrayOfMss, energyArray, '-bo', markevery = detectedClicksVisualize) # create plot with markers at detected click\n",
    "    #plt.xlabel('Milliseconds')\n",
    "    #plt.xticks(np.arange(0, max(ArrayOfMss),100))\n",
    "    #plt.ylabel('Strength')\n",
    "    #plt.title('Detecting keypresses')\n",
    "\n",
    "    #plt.show() # display plot\n",
    "    return detectedClicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3392488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    #dictionary to store data\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    #number of mfcc vectors per segment is supposed to be an integer number\n",
    "    #'0.12' is 120 ms transformed into seconds. \n",
    "    #data vectors for the AI are considered on touchPeak-20ms to touchPeak=100ms - 120ms time window\n",
    "    \n",
    "    expeceted_number_of_mfcc_vectors_per_segment = math.ceil((SAMPLE_RATE * 0.12)/hop_length)\n",
    "    \n",
    "    #Loop through all audio files\n",
    "    for root, dirnames, filenames in os.walk(DATASET_PATH):         \n",
    "        \n",
    "        #i is to label each click in a file with a label (e.g. recording: a a a a a, these all then labeled 0 0 0 0 0)\n",
    "        for i, (f) in enumerate(filenames):\n",
    "            \n",
    "            #Load audio files\n",
    "            file_path = os.path.join(root, f)\n",
    "            signal, sr = librosa.load(file_path)\n",
    "            \n",
    "            #Label data with semantic labels (mappings)\n",
    "            semantic_label = f.removesuffix(\".wav\")\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            \n",
    "            #Find the touch peaks in the audio file\n",
    "            touch_peaks = detect_touch_peaks(signal, sr)\n",
    "            \n",
    "            for peak in touch_peaks:\n",
    "                \n",
    "                #Start sample is calculated to start 20ms before the detected touch peak\n",
    "                #peak(ms)-20(ms) is devided by 1000, to get the time our start_sample started in seconds, not ms\n",
    "                #time when the sample started * sampleRate gives which sample it is (e.g. 501th sample in waveform)\n",
    "                start_sample = int(sr * (peak-20)/1000)\n",
    "                end_sample = int(sr * (peak+100)/1000)\n",
    "                \n",
    "\n",
    "                #Extract MFCC features from the signal, but only from the touch peaks (start_sample:finish_sample)\n",
    "                mfcc = librosa.feature.mfcc(signal[start_sample:end_sample],\n",
    "                                           sr=sr,\n",
    "                                           n_fft=n_fft,\n",
    "                                           n_mfcc=n_mfcc,\n",
    "                                           hop_length=hop_length)\n",
    "                #Transpose mfcc\n",
    "                mfcc = mfcc.T\n",
    "                \n",
    "                #store mfcc for segment if it has the expected length\n",
    "                if(len(mfcc) == expeceted_number_of_mfcc_vectors_per_segment):\n",
    "                    data[\"mfcc\"].append(mfcc.tolist())\n",
    "                    data[\"labels\"].append(i)\n",
    "\n",
    "    #write data into a json file                \n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_predict(predict_dataset_file_path, predict_json_path, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    #dictionary to store data\n",
    "    predict_data = {\n",
    "        \"mfcc\": [],\n",
    "    }\n",
    "    #number of mfcc vectors per segment is supposed to be an integer number\n",
    "    expeceted_number_of_mfcc_vectors_per_segment = math.ceil((SAMPLE_RATE * 0.12)/hop_length)\n",
    "    \n",
    "\n",
    "    #Load audio file\n",
    "    signal, sr = librosa.load(predict_dataset_file_path)\n",
    "\n",
    "\n",
    "    #Find the touch peaks in the audio file\n",
    "    touch_peaks = detect_touch_peaks(signal, sr)\n",
    "    \n",
    "    for peak in touch_peaks:\n",
    "\n",
    "        #Start sample is calculated to start 20ms before the detected touch peak\n",
    "        #peak(ms)-20(ms) is devided by 1000, to get the time our start_sample started in seconds, not ms\n",
    "        #time when the sample started * sampleRate gives which sample it is (e.g. 501th sample in waveform)\n",
    "        start_sample = int(sr * (peak-20)/1000)\n",
    "        end_sample = int(sr * (peak+100)/1000)\n",
    "\n",
    "\n",
    "        #Extract MFCC features from the signal, but only from the touch peaks (start_sample:finish_sample)\n",
    "        mfcc = librosa.feature.mfcc(signal[start_sample:end_sample],\n",
    "                                   sr=sr,\n",
    "                                   n_fft=n_fft,\n",
    "                                   n_mfcc=n_mfcc,\n",
    "                                   hop_length=hop_length)\n",
    "        #Transpose mfcc\n",
    "        mfcc = mfcc.T\n",
    "\n",
    "        #store mfcc for segment if it has the expected length\n",
    "        if(len(mfcc) == expeceted_number_of_mfcc_vectors_per_segment):\n",
    "            predict_data[\"mfcc\"].append(mfcc.tolist())\n",
    "\n",
    "    with open(predict_json_path, \"w\") as fp:\n",
    "        json.dump(predict_data, fp, indent=4)\n",
    "    return predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a04eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_dataset_path):\n",
    "    with open(json_dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        \n",
    "    #Convert lists into np arrays\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3823add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping_for_predict(JSON_PATH):\n",
    "    with open(JSON_PATH, \"r\") as jp:\n",
    "        data_for_maping = json.load(jp)\n",
    "    mapping = np.array(data_for_maping[\"mapping\"])\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e643061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    #Load data\n",
    "    inputs, targets = load_data(JSON_PATH)\n",
    "\n",
    "    #Split data into train and test sets\n",
    "    inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs,\n",
    "                                                                             targets,\n",
    "                                                                             test_size=0.3)\n",
    "    #Building NN architecture\n",
    "    model = keras.Sequential([\n",
    "\n",
    "        #Input layer\n",
    "        #Flatten inputs into a 1D array\n",
    "        #[0] - segments of audio [1] - intervals within segments, [2] - mfcc values for those intervals\n",
    "        keras.layers.Flatten(input_shape=(inputs.shape[1], inputs.shape[2])), \n",
    "\n",
    "        #1st hidden layer\n",
    "        keras.layers.Dense(400, activation=\"relu\"),\n",
    "\n",
    "        #2nd hidden layer\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "\n",
    "        #2nd hidden layer\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "\n",
    "        #Output layer\n",
    "        keras.layers.Dense(4, activation=\"softmax\")\n",
    "    ])\n",
    "    #Compile NN\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss=\"sparse_categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    #Train NN\n",
    "\n",
    "    model.fit(inputs_train, targets_train, validation_data=(inputs_test, targets_test), epochs=50, batch_size=32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d208aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0653098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25780e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ecffbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c117fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
