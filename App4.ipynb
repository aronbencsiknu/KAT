{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c98be15-84c7-4597-8da9-916e06b7880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stupid fucking warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from keras.layers import Conv2D, LeakyReLU\n",
    "from keras_visualizer import visualizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b6909bf-a96d-4e85-9c68-f188e0e553d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"Training-Audio\"\n",
    "TRAIN_JSON_PATH = \"Json\\\\data.json\"\n",
    "SAMPLE_RATE = 22050\n",
    "PREDICT_JSON_PATH = \"Json\\\\predict.json\"\n",
    "#threshold = 80\n",
    "#waitAfter = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d554f58-4560-4672-969f-f2a75398bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fftAndSum(signal):\n",
    "    \n",
    "    #Transform waveform into frequency domain using fast Fourier transform\n",
    "    X = np.fft.fft(signal)\n",
    "    \n",
    "    #Get 'loudness' of the 10 ms interval\n",
    "    X_mag = np.absolute(X)\n",
    "    sum_of_fft = sum(X_mag)\n",
    "    return sum_of_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dc36486-0144-46bf-9acb-9d79dff237d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(dataset_path, json_path, threshold, waitAfter, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    #dictionary to store data\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    #number of mfcc vectors per segment is supposed to be an integer number\n",
    "    #'0.12' is 120 ms transformed into seconds. \n",
    "    #data vectors for the AI are considered on touchPeak-20ms to touchPeak=100ms - 120ms time window\n",
    "    \n",
    "    expeceted_number_of_mfcc_vectors_per_segment = math.ceil((SAMPLE_RATE * 0.12)/hop_length)\n",
    "    \n",
    "    #Loop through all audio files\n",
    "    for root, dirnames, filenames in os.walk(dataset_path):         \n",
    "        \n",
    "        #i is to label each click in a file with a label (e.g. recording: a a a a a, these all then labeled 0 0 0 0 0)\n",
    "        for i, (f) in enumerate(filenames):\n",
    "            \n",
    "            #Load audio files\n",
    "            file_path = os.path.join(root, f)\n",
    "            signal, sr = librosa.load(file_path)\n",
    "            \n",
    "            #Label data with semantic labels (mappings)\n",
    "            semantic_label = f.removesuffix(\".wav\")\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            \n",
    "            print(semantic_label + \":\")\n",
    "            #Find the touch peaks in the audio file\n",
    "            touch_peaks = detect_touch_peaks(signal, sr, threshold, waitAfter, False)\n",
    "            \n",
    "            for peak in touch_peaks:\n",
    "                \n",
    "                #Start sample is calculated to start 20ms before the detected touch peak\n",
    "                #peak(ms)-20(ms) is devided by 1000, to get the time our start_sample started in seconds, not ms\n",
    "                #time when the sample started * sampleRate gives which sample it is (e.g. 501th sample in waveform)\n",
    "                start_sample = int(sr * (peak-19)/1000)\n",
    "                end_sample = int(sr * (peak+100)/1000)\n",
    "                \n",
    "\n",
    "                #Extract MFCC features from the signal, but only from the touch peaks (start_sample:finish_sample)\n",
    "                mfcc = librosa.feature.mfcc(signal[start_sample:end_sample],\n",
    "                                           sr=sr,\n",
    "                                           n_fft=n_fft,\n",
    "                                           n_mfcc=n_mfcc,\n",
    "                                           hop_length=hop_length)\n",
    "                #Transpose mfcc\n",
    "                mfcc = mfcc.T\n",
    "                \n",
    "                #store mfcc for segment if it has the expected length\n",
    "                if(len(mfcc) == expeceted_number_of_mfcc_vectors_per_segment):\n",
    "                    data[\"mfcc\"].append(mfcc.tolist())\n",
    "                    data[\"labels\"].append(i)\n",
    "\n",
    "    #write data into a json file                \n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "        \n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59df8ced-65cb-49cc-a3d3-bb85ac6c4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_predict(predict_dataset_file_path, predict_json_path, threshold, waitAfter, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    #dictionary to store data\n",
    "    predict_data = {\n",
    "        \"mfcc\": [],\n",
    "    }\n",
    "    #number of mfcc vectors per segment is supposed to be an integer number\n",
    "    expeceted_number_of_mfcc_vectors_per_segment = math.ceil((SAMPLE_RATE * 0.12)/hop_length)\n",
    "    \n",
    "\n",
    "    #Load audio file\n",
    "    signal, sr = librosa.load(predict_dataset_file_path)\n",
    "\n",
    "\n",
    "    #Find the touch peaks in the audio file\n",
    "    touch_peaks = detect_touch_peaks(signal, sr, threshold, waitAfter, False)\n",
    "    \n",
    "    for peak in touch_peaks:\n",
    "\n",
    "        #Start sample is calculated to start 20ms before the detected touch peak\n",
    "        #peak(ms)-20(ms) is devided by 1000, to get the time our start_sample started in seconds, not ms\n",
    "        #time when the sample started * sampleRate gives which sample it is (e.g. 501th sample in waveform)\n",
    "        start_sample = int(sr * (peak-20)/1000)\n",
    "        end_sample = int(sr * (peak+100)/1000)\n",
    "\n",
    "\n",
    "        #Extract MFCC features from the signal, but only from the touch peaks (start_sample:finish_sample)\n",
    "        mfcc = librosa.feature.mfcc(signal[start_sample:end_sample],\n",
    "                                   sr=sr,\n",
    "                                   n_fft=n_fft,\n",
    "                                   n_mfcc=n_mfcc,\n",
    "                                   hop_length=hop_length)\n",
    "        #Transpose mfcc\n",
    "        mfcc = mfcc.T\n",
    "\n",
    "        #store mfcc for segment if it has the expected length\n",
    "        if(len(mfcc) == expeceted_number_of_mfcc_vectors_per_segment):\n",
    "            predict_data[\"mfcc\"].append(mfcc.tolist())\n",
    "\n",
    "    with open(predict_json_path, \"w\") as fp:\n",
    "        json.dump(predict_data, fp, indent=4)\n",
    "    return predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65e32e11-66bc-491c-9809-e11a7e29212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_dataset_path):\n",
    "    with open(json_dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        \n",
    "    #Convert lists into np arrays\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21d45b4a-79e0-4de9-8931-8e4ec7d894c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping_for_predict(JSON_PATH):\n",
    "    with open(JSON_PATH, \"r\") as jp:\n",
    "        data_for_maping = json.load(jp)\n",
    "    mapping = np.array(data_for_maping[\"mapping\"])\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94cc52ae-e429-4e71-a9ee-cdc1e54e358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(test_size, validation_size):\n",
    "    \n",
    "    #Load data\n",
    "    inputs, targets = load_data(TRAIN_JSON_PATH)\n",
    "    \n",
    "    #Create train/test arrays\n",
    "    inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs, targets, test_size=test_size)\n",
    "    \n",
    "    #Create train/validation split\n",
    "    inputs_train, inputs_validation, targets_train, targets_validation = train_test_split(inputs_train, \n",
    "                                                                                          targets_train, \n",
    "                                                                                          test_size=validation_size)\n",
    "    \n",
    "    #to use data for a CNN we need to convert arrays into 3d arrays to imitate being an image\n",
    "    inputs_train=inputs_train[..., np.newaxis] # 4d array\n",
    "    inputs_validation=inputs_validation[..., np.newaxis]\n",
    "    inputs_test=inputs_test[..., np.newaxis]\n",
    "\n",
    "    return inputs_train, inputs_validation, inputs_test, targets_train, targets_validation, targets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3849cef4-5fb6-4092-8c37-061411215548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(outputs):\n",
    "    \n",
    "    inputs_train, inputs_validation, inputs_test, targets_train, targets_validation, targets_test = prepare_datasets(0.25, 0.2)\n",
    "    input_shape = (inputs_train.shape[1], inputs_train.shape[2], inputs_train.shape[3])\n",
    "    \n",
    "    #Create model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    #1st conv layer         kernals  grid_size\n",
    "    model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((3,3), strides=(1,1), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    #2nd conv layer\n",
    "    #model.add(keras.layers.Conv2D(64, (2,2), activation=LeakyReLU(), input_shape=input_shape))\n",
    "    #model.add(keras.layers.MaxPool2D((2,2), strides=(1,1), padding='same'))\n",
    "    #model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    #3rd conv layer\n",
    "    #model.add(keras.layers.Conv2D(32, (2,2), activation='relu', input_shape=input_shape))\n",
    "    #model.add(keras.layers.MaxPool2D((2,2), strides=(2,2), padding='same'))\n",
    "    #model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    #flatten the output and feed it into dense layer for classification\n",
    "    model.add(keras.layers.Flatten())\n",
    "    #model.add(keras.layers.Dense(64, activation=LeakyReLU()))\n",
    "    model.add(keras.layers.Dropout(0.3)) #lessen overfitting\n",
    "    \n",
    "    #output layer\n",
    "    model.add(keras.layers.Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    #Compile CNN\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss=\"sparse_categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    #Train CNN\n",
    "    model.fit(inputs_train, targets_train, \n",
    "              validation_data=(inputs_validation, targets_validation), \n",
    "              batch_size=8, \n",
    "              epochs=50)\n",
    "    \n",
    "    #Evaluate the CNN on the test set\n",
    "    test_error, test_accuracy = model.evaluate(inputs_test, targets_test, verbose=1)\n",
    "    window.Element(\"-OUTPUT-\").update(\"\\n\\nAccuracy on test set is: {} \\n\".format(test_accuracy), append=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d08b6b3-0209-4644-9e7d-533d1fac645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_touch_peaks(signal, sr, threshold, waitAfter, drawGraph):\n",
    "    \n",
    "    # splitting sound into 10 ms chunks\n",
    "    soundLength = len(signal) / sr # length in seconds\n",
    "    howManyParts = soundLength * 100 # how many chunks to cut up into\n",
    "    tenMsParts = np.array_split(signal, howManyParts) # create new array containing 10ms parts\n",
    "    \n",
    "    currentTime = 0\n",
    "    energyArray = []\n",
    "    detectedClicks = []\n",
    "    detectedClicksVisualize = [] #Used for plotting FFTs\n",
    "    ArrayOfMss = []\n",
    "    passedFirstPeak = False\n",
    "    detectedKeyPressTime = 0\n",
    "\n",
    "    for oneSegment in tenMsParts:\n",
    "        \n",
    "        ArrayOfMss.append(currentTime) # append current time\n",
    "        sum_of_fft = round(fftAndSum(oneSegment)) # call fftAndSum and store returned value\n",
    "        energyArray.append(sum_of_fft)\n",
    "\n",
    "        #sum_of_fft > (Threshold) which is considered as a peak\n",
    "        if(sum_of_fft > threshold and not(passedFirstPeak)):\n",
    "            \n",
    "            #passedFirstPeak bool is used to skip over the button's release peak when detected the touch peak\n",
    "            passedFirstPeak = True\n",
    "            detectedKeyPressTime = currentTime\n",
    "            \n",
    "            detectedClicks.append(currentTime) # add time of detected click\n",
    "            detectedClicksVisualize.append(int(currentTime/10))\n",
    "            \n",
    "        elif(currentTime - detectedKeyPressTime > waitAfter): # wait x milliseconds after first peak\n",
    "            passedFirstPeak = False\n",
    "            \n",
    "        currentTime += 10\n",
    "    \n",
    "    if drawGraph == True:\n",
    "        prepareAndOpenGraphWindow(ArrayOfMss, energyArray, detectedClicksVisualize)\n",
    "    \n",
    "    return detectedClicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f38fc8f0-e08c-493e-99b9-97441eae9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PySimpleGUI as sg\n",
    "import os\n",
    "import traceback\n",
    "import sys\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import wavio as wv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a97c3ed-caed-4066-b1df-0f993e29fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareAndOpenGraphWindow(ArrayOfMss, energyArray, detectedClicksVisualize):\n",
    "    window = sg.Window(\"Graph\", \n",
    "    [[sg.Text('Audio peaks and touch peaks')],\n",
    "    [sg.Canvas(key='-CANVAS-')]],\n",
    "    finalize=True, element_justification='center', font='Monospace 18')\n",
    "    \n",
    "    figure = plt.figure(figsize=(20, 8))\n",
    "    plt.plot(ArrayOfMss, energyArray, '-bo', markevery = detectedClicksVisualize) # create plot with markers at detected click\n",
    "    plt.xlabel('Milliseconds')\n",
    "    plt.xticks(np.arange(0, max(ArrayOfMss),1000))\n",
    "    plt.ylabel('Strength')\n",
    "    plt.title('Detecting keypresses')\n",
    "    \n",
    "    fig_agg = draw_figure(window['-CANVAS-'].TKCanvas, figure)\n",
    "    \n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == \"Exit\" or event == sg.WIN_CLOSED:\n",
    "            break\n",
    "        \n",
    "    window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b835bb5e-3ae4-4164-b75c-5dfba3074060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_models():\n",
    "    numOfFiles = 0\n",
    "    directory = os.getcwd()\n",
    "    for File in os.listdir(directory):\n",
    "        if File.endswith(\".pb\"):\n",
    "            os.remove(File)\n",
    "            numOfFiles += 1\n",
    "    window.Element(\"-OUTPUT-\").update(str(numOfFiles) + \" files deleted \\n\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f8197e1-d074-4fc8-b3f8-616288bc9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTrainingFiles():\n",
    "    directory = values[\"-TRAINING_DIRECTORY-\"]\n",
    "    for File in os.listdir(directory):\n",
    "        if File.endswith(\".wav\"):  \n",
    "            filename = os.fsdecode(File)\n",
    "            print(filename)\n",
    "            window.Element(\"-OUTPUT-\").update(filename + \"\\n\", append=True)\n",
    "            window.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a40ee58a-6929-416e-be1d-659f1d49af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelExists():\n",
    "    directory = os.getcwd()\n",
    "    for File in os.listdir(directory):\n",
    "        if File.endswith(\".pb\"):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd31d502-4fa6-4ee1-979e-3588f67c5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareAndTrainModel():\n",
    "    window.Element(\"-OUTPUT-\").update(\"Saving data to JSON... \\n\", append=True)\n",
    "    outputs = save_data(values[\"-TRAINING_DIRECTORY-\"], TRAIN_JSON_PATH, values[\"-TRAINING_THRESHOLD-\"], values[\"-TRAINING_WAIT_AFTER-\"])\n",
    "    window.Element(\"-OUTPUT-\").update(\"Training model... \\n\", append=True)\n",
    "    print(outputs)\n",
    "    model = build_model(outputs)\n",
    "    window.Element(\"-OUTPUT-\").update(\"Saving model... \\n\", append = True)\n",
    "    model.save(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f00120c4-62cc-4a8b-b66a-f4f7d93bac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareAndTestModel():\n",
    "    window.Element(\"-OUTPUT-\").update(\"\\n\", append=True)\n",
    "    tmp = save_data_to_predict(values[\"-TESTING_DIRECTORY-\"], PREDICT_JSON_PATH, values[\"-TESTING_THRESHOLD-\"], values[\"-TESTING_WAIT_AFTER-\"])\n",
    "    data = np.array(tmp[\"mfcc\"])\n",
    "    mapping = load_mapping_for_predict(TRAIN_JSON_PATH)\n",
    "    model = keras.models.load_model(os.getcwd())\n",
    "    predictions = model.predict(data)\n",
    "    predicted_index = np.argmax(predictions, axis=1)\n",
    "    window.Element(\"-OUTPUT-\").update(\"predicted mapping: \" + str(mapping[predicted_index]) + \"\\n\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c4ff44e-5b0a-4d77-a086-059c89daf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingDirectoryIsGood():\n",
    "    directory = values[\"-TRAINING_DIRECTORY-\"]\n",
    "    if os.path.isdir(directory):\n",
    "        for File in os.listdir(directory):\n",
    "            if File.endswith(\".wav\"):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2af8ed76-23cf-4e01-a65c-66d869ec9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingDirectoryIsGood():\n",
    "    directory = values[\"-TESTING_DIRECTORY-\"]\n",
    "    if os.path.isfile(directory):\n",
    "        if directory.endswith(\".wav\"):\n",
    "            return True\n",
    "        return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "653ed308-4d33-436b-b77f-71b5be300445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareAndStartRecording():\n",
    "    freq = 44100\n",
    "    duration = values[\"-RECORDING_LENGTH-\"]\n",
    "    filename = values[\"-RECORDING_FILE_NAME-\"]\n",
    "    window.Element(\"-OUTPUT-\").update(\"Recording... \\n\", append=True)\n",
    "    window.refresh()\n",
    "    recording = sd.rec(int(duration * freq), samplerate=freq, channels=2)\n",
    "    sd.wait()\n",
    "    wv.write(\"Recorder-Audio/\" + filename + \".wav\", recording, freq, sampwidth=2)\n",
    "    window.Element(\"-OUTPUT-\").update(\"\\nRecording finished! \\n\", append=True)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1dd23987-9116-43cf-9955-5930429af2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_audio():\n",
    "    numOfFiles = 0\n",
    "    directory = os.getcwd() + \"/Recorder-Audio\"\n",
    "    if not len(directory) == 0:\n",
    "        for File in os.listdir(directory):\n",
    "            path = os.path.join(directory, File)    \n",
    "            os.remove(path)\n",
    "            numOfFiles += 1\n",
    "        window.Element(\"-OUTPUT-\").update(str(numOfFiles) + \" audio files deleted... \\n\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76ac5e55-cc33-490d-96cc-efe530d22a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_figure(canvas, figure):\n",
    "    figure_canvas_agg = FigureCanvasTkAgg(figure, canvas)\n",
    "    figure_canvas_agg.draw()\n",
    "    figure_canvas_agg.get_tk_widget().pack(side='top', fill='both', expand=1)\n",
    "    return figure_canvas_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e32b84c8-f6a4-4c79-b46e-eed84768bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_definition = [\n",
    "    ['Tools', ['Trainer', 'Tester','Audio recorder']],\n",
    "    ['More', ['About', 'Clear output', 'Delete models', 'Delete audio', 'Delete all']]\n",
    "]\n",
    "                 \n",
    "menu = [\n",
    "    sg.Menu(menu_definition, tearoff=False, pad=(200, 1), key=\"-MENU-\")\n",
    "]\n",
    "\n",
    "middle_train = [\n",
    "    [\n",
    "        sg.Text(\"TRAINER\", font = ('any 14'))\n",
    "    ],\n",
    "    [\n",
    "        sg.Canvas(key='figCanvas')\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Training data folder:\"),\n",
    "        sg.In(size=(25, 1), enable_events=True, key=\"-TRAINING_DIRECTORY-\",disabled=True),\n",
    "        sg.FolderBrowse(\"Open folder with files\"),\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Threshold: \"),\n",
    "        sg.Slider((1,300), key='-TRAINING_THRESHOLD-', orientation='h', enable_events=True, disable_number_display=False)\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Wait after each peak (ms): \"),\n",
    "        sg.Slider((1,200), key='-TRAINING_WAIT_AFTER-', orientation='h', enable_events=True, disable_number_display=False)\n",
    "    ],\n",
    "    [\n",
    "        sg.Button(\"TRAIN\", enable_events=True, key=\"-START_TRAINING-\"),\n",
    "    ]\n",
    "]\n",
    "\n",
    "middle_test = [\n",
    "    [\n",
    "        sg.Text(\"TESTER\", font = ('any 14'))\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Test file:\"),\n",
    "        sg.In(size=(25, 1), enable_events=True, key=\"-TESTING_DIRECTORY-\",disabled=True),\n",
    "        sg.FileBrowse(\"Open file\"),\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Threshold: \"),\n",
    "        sg.Slider((1,300), key='-TESTING_THRESHOLD-', orientation='h', enable_events=True, disable_number_display=False)\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Wait after each peak (ms): \"),\n",
    "        sg.Slider((1,200), key='-TESTING_WAIT_AFTER-', orientation='h', enable_events=True, disable_number_display=False)\n",
    "    ],\n",
    "    [\n",
    "        sg.Button(\"TEST\", enable_events=True, key=\"-START_TESTING-\"),\n",
    "    ]\n",
    "]\n",
    "\n",
    "middle_record = [\n",
    "    [\n",
    "        sg.Text(\"RECORDER\", font = ('any 14'))\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Audio file name: \"),\n",
    "        sg.In(size = (25, 1), enable_events=True, key=\"-RECORDING_FILE_NAME-\")\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Recording length: \"),\n",
    "        sg.Slider((3,30), key='-RECORDING_LENGTH-', orientation='h', enable_events=True, disable_number_display=False)\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Threshold: \"),\n",
    "        sg.Slider((1,300), key='-RECORDING_THRESHOLD-', orientation='h', enable_events=True, disable_number_display=False)\n",
    "    ],\n",
    "    [\n",
    "        sg.Text(\"Wait after each peak (ms): \"),\n",
    "        sg.Slider((1,200), key='-RECORDING_WAIT_AFTER-', orientation='h', enable_events=True, disable_number_display=False)\n",
    "    ],\n",
    "    [\n",
    "        sg.Button(\"Record\", enable_events=True, key=\"-START_RECORDING-\"),\n",
    "    ],\n",
    "]\n",
    "\n",
    "bottom = [\n",
    "    [\n",
    "        sg.Text(\"Output:\")\n",
    "    ],\n",
    "    [\n",
    "        sg.Multiline(size = (80, 20), enable_events=True, key=\"-OUTPUT-\")\n",
    "    ]\n",
    "]\n",
    "\n",
    "layout = [menu, \n",
    "         [sg.Column(middle_train, key='-layout_train-'), \n",
    "          sg.Column(middle_test, visible=False, key='-layout_test-'), \n",
    "          sg.Column(middle_record,visible=False, key='-layout_record-')],\n",
    "          bottom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedf552-a945-4478-93dc-81ba80db79ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.wav\n",
      "space.wav\n",
      "a:\n",
      "space:\n",
      "2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 4, 11, 32)         320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 4, 11, 32)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 4, 11, 32)        128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1408)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1408)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 2818      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,266\n",
      "Trainable params: 3,202\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 9ms/step - loss: 1.1738 - accuracy: 0.4817 - val_loss: 0.7497 - val_accuracy: 0.6585\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.7942 - accuracy: 0.5244 - val_loss: 0.5618 - val_accuracy: 0.8049\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.6890 - val_loss: 0.4449 - val_accuracy: 0.8537\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.5315 - accuracy: 0.7134 - val_loss: 0.3704 - val_accuracy: 0.9024\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.4948 - accuracy: 0.7622 - val_loss: 0.3210 - val_accuracy: 0.9268\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.4455 - accuracy: 0.7988 - val_loss: 0.2771 - val_accuracy: 0.9268\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.3410 - accuracy: 0.8476 - val_loss: 0.2486 - val_accuracy: 0.9268\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.3489 - accuracy: 0.8598 - val_loss: 0.2290 - val_accuracy: 0.9268\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8537 - val_loss: 0.2080 - val_accuracy: 0.9268\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.2517 - accuracy: 0.9268 - val_loss: 0.1929 - val_accuracy: 0.9268\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2668 - accuracy: 0.9024 - val_loss: 0.1797 - val_accuracy: 0.9512\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.2268 - accuracy: 0.9329 - val_loss: 0.1663 - val_accuracy: 0.9268\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.2283 - accuracy: 0.9390 - val_loss: 0.1530 - val_accuracy: 0.9512\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.2027 - accuracy: 0.9329 - val_loss: 0.1445 - val_accuracy: 0.9512\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.1514 - accuracy: 0.9634 - val_loss: 0.1366 - val_accuracy: 0.9756\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9329 - val_loss: 0.1295 - val_accuracy: 0.9756\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9268 - val_loss: 0.1196 - val_accuracy: 0.9756\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.1714 - accuracy: 0.9390 - val_loss: 0.1127 - val_accuracy: 0.9756\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9817 - val_loss: 0.1064 - val_accuracy: 0.9756\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9634 - val_loss: 0.1052 - val_accuracy: 0.9756\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.1157 - accuracy: 0.9695 - val_loss: 0.0950 - val_accuracy: 0.9756\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9817 - val_loss: 0.0891 - val_accuracy: 0.9756\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.9695 - val_loss: 0.0853 - val_accuracy: 0.9756\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0897 - accuracy: 0.9756 - val_loss: 0.0828 - val_accuracy: 0.9756\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.1033 - accuracy: 0.9756 - val_loss: 0.0779 - val_accuracy: 0.9756\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0851 - accuracy: 0.9817 - val_loss: 0.0735 - val_accuracy: 0.9756\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0717 - accuracy: 0.9817 - val_loss: 0.0694 - val_accuracy: 0.9756\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0875 - accuracy: 0.9878 - val_loss: 0.0659 - val_accuracy: 0.9756\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.9634 - val_loss: 0.0624 - val_accuracy: 0.9756\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9817 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 0.9817 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9756 - val_loss: 0.0538 - val_accuracy: 0.9756\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.9878 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9756 - val_loss: 0.0468 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0677 - accuracy: 0.9817 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0694 - accuracy: 0.9939 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0533 - accuracy: 0.9878 - val_loss: 0.0411 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0532 - accuracy: 0.9878 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9939 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0479 - accuracy: 0.9878 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.9878 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0397 - accuracy: 0.9939 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0521 - accuracy: 0.9817 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9756 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.0461 - accuracy: 0.9939 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.0552 - accuracy: 0.9878 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0452 - accuracy: 0.9817 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.0538 - accuracy: 0.9939 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.9710\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\creep\\KAT-CNN-Test\\assets\n"
     ]
    }
   ],
   "source": [
    "window = sg.Window(\"KAT\", layout)\n",
    "while True:\n",
    "    try:\n",
    "        event, values = window.read()\n",
    "        \n",
    "        if event == sg.WIN_CLOSED:\n",
    "            break\n",
    "        \n",
    "        if event == \"-TRAINING_DIRECTORY-\":\n",
    "            if trainingDirectoryIsGood():\n",
    "                printTrainingFiles()\n",
    "                window.Element(\"-OUTPUT-\").update(\"Training directory is fine! You can proceed to train the model... \\n\", append=True)\n",
    "            else:\n",
    "                window.Element(\"-OUTPUT-\").update(\"No .wav files detected in directory...\\n\", append=True)\n",
    "                \n",
    "        if event == \"-TESTING_DIRECTORY-\":\n",
    "            if testingDirectoryIsGood():\n",
    "                window.Element(\"-OUTPUT-\").update(\"Testing file is good! \\n\", append=True)\n",
    "            else:\n",
    "                window.Element(\"-OUTPUT-\").update(\"Not a .wav file. Make sure to find a .wav file! \\n\", append=True)\n",
    "                \n",
    "        if event == \"-START_TRAINING-\":\n",
    "            if trainingDirectoryIsGood():\n",
    "                window.perform_long_operation(lambda: prepareAndTrainModel(), \"-MODEL_TRAINED-\")\n",
    "            else:\n",
    "                window.Element(\"-OUTPUT-\").update(\"Error. Training Directory does not contain any .wav files... \\n\", append=True)\n",
    "        \n",
    "        if event == \"-MODEL_TRAINED-\":\n",
    "            window.Element(\"-OUTPUT-\").update(\"Model succesfully trained and saved. You can now test it! (Check the menu up top) \\n\", append=True)\n",
    "       \n",
    "        if event == \"-START_TESTING-\":\n",
    "            if testingDirectoryIsGood():\n",
    "                if modelExists():\n",
    "                    window.Element(\"-OUTPUT-\").update(\"Testing model... \\n\", append=True)\n",
    "                    window.perform_long_operation(lambda: prepareAndTestModel(), \"-MODEL_TESTED-\")\n",
    "                else:\n",
    "                    window.Element(\"-OUTPUT-\").update(\"No model currently exists. Please load or train a new model \\n\", append=True)\n",
    "            else:\n",
    "                window.Element(\"-OUTPUT-\").update(\"Bad directory. Make sure you selected a .wav file! \\n\", append=True)\n",
    "              \n",
    "        if event == \"-MODEL_TESTED-\":\n",
    "            window.Element(\"-OUTPUT-\").update(\"End of testing. \\n\", append=True)       \n",
    "        \n",
    "        if event == \"-START_RECORDING-\":\n",
    "            if not values[\"-RECORDING_FILE_NAME-\"] == \"\":\n",
    "                if len(str(values[\"-RECORDING_FILE_NAME-\"])) < 10:\n",
    "                    window.perform_long_operation(lambda: prepareAndStartRecording(), \"-RECORDING_FINISHED-\")\n",
    "                else:\n",
    "                    window.Element(\"-OUTPUT-\").update(\"Name too long... \\n\", append=True)\n",
    "            else:\n",
    "                window.Element(\"-OUTPUT-\").update(\"Please enter a name for your recording... \\n\", append=True)\n",
    "\n",
    "        if event == \"-RECORDING_FINISHED-\":\n",
    "            signal, sr = librosa.load(\"Recorder-Audio/\" + str(values[\"-RECORDING_FINISHED-\"]) + \".wav\")\n",
    "            touch_peaks = detect_touch_peaks(signal, sr, values[\"-RECORDING_THRESHOLD-\"], values[\"-RECORDING_WAIT_AFTER-\"], True)\n",
    "    \n",
    "        if event == \"Trainer\":\n",
    "            window['-layout_test-'].update(visible=False)\n",
    "            window['-layout_train-'].update(visible=True)\n",
    "            window['-layout_record-'].update(visible=False)\n",
    "            \n",
    "        if event == \"Tester\":\n",
    "            window['-layout_train-'].update(visible=False)\n",
    "            window['-layout_test-'].update(visible=True)\n",
    "            window['-layout_record-'].update(visible=False)\n",
    "            \n",
    "        if event == \"Audio recorder\":\n",
    "            window['-layout_record-'].update(visible=True)\n",
    "            window['-layout_test-'].update(visible=False)\n",
    "            window['-layout_train-'].update(visible=False)\n",
    "            \n",
    "        if event == 'About':\n",
    "            sg.Popup(\n",
    "                \"Welcome to KAT. This is a simple UI for KAT - Keyboard Accoustic Translator\",\n",
    "                \"This ui is simple to use:\",\n",
    "                \"1. Record some audio with the recorder tool (make sure you see bubbles on spikes)\",\n",
    "                \"2. Train the data via trainer tool(change sliders accordingly)\",\n",
    "                \"3. Record a test file\",\n",
    "                \"4. Test it on the tester tool\"\n",
    "            )\n",
    "        if event == 'Clear output':\n",
    "            window.Element(\"-OUTPUT-\").update(\"\")\n",
    "            \n",
    "        if event == \"Delete models\":\n",
    "            cleanup_models()\n",
    "        \n",
    "        if event == \"Delete audio\":\n",
    "            cleanup_audio()\n",
    "            \n",
    "        if event == \"Delete all\":\n",
    "            window.Element(\"-OUTPUT-\").update(\"\")\n",
    "            cleanup_models()\n",
    "            cleanup_audio()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(traceback.format_exc())\n",
    "        break\n",
    "        \n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734c2be-98d8-4f3a-82f2-e8a64bb29899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ab5a6-360f-4bec-9925-9d106103e30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6c6da-d698-41da-9698-c57c27dfaf85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2ecb3-7bcd-4530-9e43-00df9f00d5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
